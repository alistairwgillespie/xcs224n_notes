{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCS224N Natural Language Processing with Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS224N](http://web.stanford.edu/class/cs224n/) / [XCS224N](http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&courseId=93933715) / [Lecture](https://youtu.be/kEMJRjEdNzM) / [Slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing Word2Vec once more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word2vec_example.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c) = \\frac{exp(u_{O}^{T}v_{c})}{\\sum_{w\\in V}exp(u_{W}^{T}v_{c})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec parameters and computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most libraries, such as TensorFlow and PyTorch, word vectors are stored as rows, contrary to being stored as columns as seen in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/matrices.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from Word2Vec is a probability distribution for all words, assigning reasonably high probabilities to words that often occur in the context of a given center word.\n",
    "\n",
    "However, it is important to consider common words such as <strong>\"the\"</strong>, <strong>\"and\"</strong>, and so on. They are very common as context words and will have a high dot product value. This can be referred to as the <strong>high frequency probability effect</strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec maximizes objective function by putting similar words nearby in space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word_vector_example.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drawback to these visualisations though. Words can be close to one another in many different directions (across multiple dimensions). This is where 2 dimensional projections can be misleading, and may only show a small part of the relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cost function $J(\\theta)$ we would like to minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing <strong>gradient descent</strong> we can minimize $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is:\n",
    "* For the current $\\theta$, calculate the gradient of $J(\\theta)$\n",
    "* Then take a small step in the direction of the negative gradient (i.e. walking down the hill)\n",
    "* Repeat until optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient_descent.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the equation (in matrix notation) is done by via the step size or learning (<strong>$\\alpha$</strong>):\n",
    "\n",
    "$$\\theta^{new} = \\theta^{old} - \\alpha\\bigtriangledown_\\theta J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the equation for a single parameter is done via the following equation:\n",
    "\n",
    "$$\\theta_{j}^{new} = \\theta_{j}^{old} - \\alpha \\frac{\\partial}{\\partial\\theta_{j}^{old}} J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "```python \n",
    "while true:\n",
    "    theta_grad = evaluate gradient(J, corpus, theta)\n",
    "    theta = theta - alpha * theta_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example where you will likely need to optimize the gradient for billions of records, the approach of gradient desecent for all windows in the corpus becomes too expensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Problem</strong>: $J(\\theta)$ is a function of <strong>all</strong> windows, for example, in the billions in some cases depending on the training dataset. Thus, $\\bigtriangledown_\\theta J(\\theta)$ can be too expensive to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Solution</strong>: <strong>Stochastic gradient descent (SGD)</strong> by batching sample windows, and updating the cost function after each sample in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach for <strong>SGD</strong> with Word Vectors:\n",
    "* Iterate taking gradients at each such window for SGD\n",
    "* In each window, we only have 2m + 1 words, so $\\bigtriangledown_\\theta J_{t}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bigtriangledown_\\theta J_{t}(\\theta) =  \n",
    "\\begin{pmatrix}\n",
    "  0 \\\\\n",
    "  \\vdots \\\\\n",
    "  \\bigtriangledown_{v_{like}} \\\\\n",
    "  \\vdots \\\\\n",
    "  0 \\\\\n",
    "  \\bigtriangledown_{u_{I}} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\bigtriangledown_{u_{learning}} \\\\\n",
    "  \\vdots\n",
    " \\end{pmatrix}\n",
    " \\in \\mathbb{R}^{2dV}\n",
    " $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just about all elements in the $\\bigtriangledown_\\theta J_{t}(\\theta)$ vector are 0, in other words it is a <strong>sparse vector</strong>. This differs from other use cases in deep learning such as computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Problem</strong>: This poses the problem of very sparse parameter updating. Thus, we probably only want to update the non-zero vectors but this can be complex and may require matrix operations and strategies to accomodate distributed computation to get parameter estimates.\n",
    "\n",
    "<strong>Solution</strong>:\n",
    "* Sparse matrix update operations to only update the certain rows of full embedding matrics U and V\n",
    "* Hash for word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Model Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why two vectors? Easier optimization, however it can be done with just one vector per word.\n",
    "\n",
    "Two model variants:\n",
    "1. Skip-grams (SG) - <em>The presented model here</em>\n",
    "2. Continuous Bag of Words (CBOW): Predict center word from (bag of) context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Skip-gram model with negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get additional efficiency in training, we adopt <strong>negative sampling</strong>. So far, we have focused on <strong>naive softmax</strong> which is simpler and a more computationally expensive training method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c) = \\frac{exp(u_{O}^{T}v_{c})}{\\sum_{w\\in V}exp(u_{W}^{T}v_{c})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator or normalization factor requires you to sum over the entire vocabulary (i.e. all words in the vocabulary). This also means for however many words in the vocabulary we will need to compute the same amount of dot products. This is too computationally expensive, hence <strong>negative sampling (Mikolov and colleagues)</strong> was born."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to train binary logistic regressions for a true pair (center word and word in its context window) versus several noise pairs (the center word paired with a random word).\n",
    "* Assigning high probabilities to words that were actually seen (true pairs)\n",
    "* Assigning low probabilities to words (randomly sampled) that were not seen (noise pairs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Distributed Representations of Words and Phrases and their Compositionality” (Mikolov et al. 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation to be used for negative sampling in XCS224N (differing to the one in Mikolov et al. 2013):\n",
    "\n",
    "$$J_{neg - sample}(o,v_{c},U) = -log(\\sigma(u_{o}^{T}v_{c})) - \\sum_{k=1}^{K}log(\\sigma(-u_{k}^{T}v_{c}))$$\n",
    "\n",
    "* Maximising probability of real outside words that appear i.e. in two words co-occurring with first log\n",
    "* Penalising/minimizing probability that random words appear around center word i.e. taking k negative samples (using word probabilities)\n",
    "\n",
    "$$P(w) = \\frac{U(w)^{3/4}}{Z}$$\n",
    "\n",
    "* Unigram distribution raised to the 3/4 power ensures common words are penalised and uncommon words maxmimized\n",
    "* Z being the normalizing term\n",
    "* w being the count of the word for every word in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to Word2vec, we use a window around each word to capture both semantic and syntactic (Part of Speech) meaning.\n",
    "\n",
    "The result being a window-based co-occurence matrix with the following properties and problems:\n",
    "* Symmetric\n",
    "* Increase in size with vocabulary\n",
    "* Very high dimensionality requiring a lot of storage\n",
    "* Downstream classification models have sparsity issues\n",
    "* Models are less robust\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/co-occurence.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Solution</strong>: Store and preserve most of the informing variance in a fixed and smaller dense matrix using Dimensionality Reduction. Usually down to somewhere between 25-1000 dimensions (similar to Word2vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular Value Decomposition of the Co-occurence Matrix\n",
    "\n",
    "- Latent semantic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SVD.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacks to X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Rohde et al. 2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacks included:\n",
    "* For function words - $min(X, t)$, with $t \\approx 100$\n",
    "* For function words - ignore them all\n",
    "* Ramped windows with more emphasis on closer words in the window\n",
    "* Pearson correlations instead of counts, then set negative values to 0\n",
    "\n",
    "The result being some fairly handy word vectors.\n",
    "\n",
    "Some interesting results are generated with verbs and respective doers holding linear relatinships. The result being a model that has analogical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/semantic_similarity.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count based vs. direct prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/count_vs_predict.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about combining the two to get superior results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Encoding meaning in vector differences [Pennington, Socher, and Manning, EMNLP 2014]</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vector_difference.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Q</strong> How can we capture ratios of co-occurence probabilities as linear meaning components in a word vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>A</strong>\n",
    "\n",
    "Log-bilinear model: $$w_{i} . w_{j} = logP(i|j)$$\n",
    "\n",
    "With vector differences: $$w_{x}.(w_{a} - w_{b}) = log(\\frac{P(x|a)}{P(x|b)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "[Pennington et al., EMNLP 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_{i} . w_{j} = logP(i|j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J = \\sum_{i,j=1}^{V}f(X_{ij})(w_{i}^{T}\\tilde{w}_{j}+b_{i}+\\tilde{b}_{j} - log X_{ij})^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting in:\n",
    "* Fast training\n",
    "* Scalable to huge corpora\n",
    "* Good performance even with small vectors and corpus\n",
    "* $f$ function used to limit impact of common words also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in NLP, Intrinsic vs. Extrinsic evaluation is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Intrinsic</strong>\n",
    "\n",
    "Are synonyms grouped together?\n",
    "\n",
    "Fast to compute?\n",
    "\n",
    "How does it evaluate on specific/intermediate subtasks? e.g. analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Extrinsic</strong>\n",
    "\n",
    "Does it make the system better?\n",
    "\n",
    "How long does it take to compute?\n",
    "\n",
    "How does it perform on a real task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic Word Vector Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vector Analogies\n",
    "\n",
    "$a:b :: c:?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe paper:\n",
    "* Good dimensionality is ~300\n",
    "* Window size 8 works best for GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality of Word Embeddings [Zi Yin and Yuanyuan Shen, NeurIPS 2018]\n",
    "* Things don't fall apart at high dimensions contrary to other beliefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia data is superior to news and other data sources for training Word Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity judgements (based on word vector distances and their correlations with human judgements). May also use cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Senses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most words have lots of meanings and possess ambiguity (especially for common words or words that have existed for a long time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a single vector capture multiple meanings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving Word Representations Via Global Context And Multiple Word Prototypes (Huang et al. 2012)\n",
    "* Breaking word into its pseudowords via clusters\n",
    "* Then retrain with each word assigned to cluster i.e. $bank_{1}$, $bank_{2}$, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Algebraic Structure of Word Senses, with Applications to Polysemy (Arora,.., Ma,..., TACL 2018)\n",
    "* Difference senses of a word reside in a linear Superposition such as a Word2vec i.e. weighted average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrinsic Word Vector Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Finding a person, organization or location\n",
    "\n",
    "By just using Word2vec, the efficiency or accuracy of your system improved dramatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
