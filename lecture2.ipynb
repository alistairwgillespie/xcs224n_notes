{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCS224N Natural Language Processing with Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS224N](http://web.stanford.edu/class/cs224n/) / [XCS224N](http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&courseId=93933715) / [Lecture](https://youtu.be/kEMJRjEdNzM) / [Slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing Word2Vec once again..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word2vec_example.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c) = \\frac{exp(u_{O}^{T}v_{c})}{\\sum_{w\\in V}exp(u_{W}^{T}v_{c})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec parameters and computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most libraries, such as TensorFlow and PyTorch, word vectors are the rows contrary to columns in linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/matrices.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a probability distribution for all words, giving reasonably high probabilities to words that often occur in the context of a given word.\n",
    "\n",
    "However, what about <strong>\"the\"</strong>, <strong>\"and\"</strong>, and so on. They are very common as context words and will have a high dot product value. This can be referred to as the high frequency probability effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec maximizes objective function by putting similar words nearby in space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word_vector_example.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a drawback to these visualisations though. Words can be close to one another in many different directions (across multiple dimensions). This is where 2 projections can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cost function $J(\\theta)$ we would like to minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing <strong>gradient descent</strong> we can minimize $J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is:\n",
    "* For the current $\\theta$, calculate the gradient of $J(\\theta)$\n",
    "* Then take a small step in the direction of negative gradient (i.e. walking down the hill)\n",
    "* Repeat until optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
