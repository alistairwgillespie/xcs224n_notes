{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCS224N Natural Language Processing with Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS224N](http://web.stanford.edu/class/cs224n/) / [XCS224N](http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&courseId=93933715) / [Lecture](https://youtu.be/8CWyBNX6eDo) / [Slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture03-neuralnets.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2-3 Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to learn about:\n",
    "* Deep, multi-layer neural networks and how they can be trained using backpropagation (matrix calculus)\n",
    "* NLP classifiers that add context, by taking in windows of words and classifying a center word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Setup and Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training dataset consisting of samples $\\{x_i, y_i\\}^N$\n",
    "* $x_i$ are inputs (dimension d)\n",
    "* $y_i$ are labels we try to predict (one of C classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional ML/Stats approach:\n",
    "* Assume $x_i$ are fixed (i.e. set), \n",
    "* Train softmax/logistic regression weights $W\\in \\mathbb{R}^{Cxd}$ \n",
    "* By way of training, determine a decision boundary (hyperplane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<img src=\"images/decision_boundary.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y|x) = \\frac{exp(W_{y}.x)}{\\sum_{c=1}^{C}exp(W_{c}.x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take the $y^{th}$ row of W and multiply that row with x (dot product)\n",
    "$$ W_{y}.x = \\sum_{i=1}^{d}W_{yi}x_{i}=f_{y}$$\n",
    "\n",
    "For Weight matrix $W$, we have a row corresponding to each class. Then for that row, we are dot producting it with our datapoint vector $x_i$. This then gives a score for how likely it is that the example belongs to a class.\n",
    "\n",
    "2. Apply softmax to get normalized probability\n",
    "\n",
    "$$P(y|x) = \\frac{exp(f_{y})}{\\sum_{c=1}^{C}exp(f_{c})}=softmax(f_{y})$$\n",
    "\n",
    "In statistics, the purists may think that we are storing more weights than we need, and rather we only need N - 1 for classification i.e. for binary classification we only need one weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Goal</strong>\n",
    "<br>\n",
    "\n",
    "i) For every training example $(x,y)$, the objective is to <em>maximize the probability of the correct class $y$\n",
    "</em>\n",
    "<br>\n",
    "ii) Or, <em>minimize the negative log probability of that class</em>:\n",
    "<br>\n",
    "$$-log p(y|x)=-log(\\frac{exp(f_y)}{\\sum_{c=1}^{C}exp(f_{c})})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is \"cross entropy\" loss/error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Originates from information theory\n",
    "* Let the true probability distribution be $p$\n",
    "* Let our computer probability be $q$\n",
    "* Then cross entropy is:\n",
    "<br>\n",
    "$$ H(p,q) = -\\displaystyle\\sum_{c=1}^{C}p(c) log q(c)$$\n",
    "<br>\n",
    "* Assuming ground truth probability distribution that is 1 at the right class 0 everywhere else (i.e. one hot encoded vector):\n",
    "```python\n",
    "p = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
    "```\n",
    "<strong>Because of one-hot $p$, the only term left is the negative log probability of the true class</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Q: What is the p vector? What do each of the components correspond to? </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification over a full dataset\n",
    "* Differs for classification over a full dataset $\\{x_i, y_i\\}^N_{i=1}$\n",
    "<br>\n",
    "$$J(\\theta) = \\frac{1}{N} \\displaystyle\\sum_{i=1}^{N}-log(\\frac{e^{f_{y_{i}}}}{\\sum_{c=1}^{C}exp(e^{f_{c}})})$$\n",
    "* Instead of:\n",
    "<br>\n",
    "$$f_y = f_y(x) = W_y.x = \\displaystyle\\sum_{j=1}^{d}W_{yj}x_j$$\n",
    "\n",
    "* We will write $f$ in matrix notation:\n",
    "$$f=Wx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $W$ matrix will have <strong>$C$ rows equal to the number of classes</strong> , and <strong>$d$ columns equal to the number of dimensions of the input vector $x$</strong>, thus:\n",
    "<br>\n",
    "<br>\n",
    "$W\\in \\mathbb{R}^{Cxd}$ \n",
    "\n",
    "For each $y$ row that corresponds to a class in the $W$ matrix: \n",
    "* We take the dot product of the $yth$ and $jth$ component of $W$ and $jth$ component of $x$ for all input dimensions\n",
    "* Then take the sum\n",
    "* This is simplified to $f_y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional ML optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general ML, $\\theta$ usually only consists of columns of W:\n",
    "$$\\theta = \n",
    "\\begin{pmatrix}\n",
    "  W_{.1} \\\\\n",
    "  \\vdots \\\\\n",
    "  W_{.d} \\\\\n",
    " \\end{pmatrix}\n",
    " =W(:) \\in \\mathbb{R}^{Cd}$$\n",
    "<br>\n",
    "So we only update the decision boundary via:\n",
    "$$\\bigtriangledown_\\theta J(\\theta) = \n",
    "\\begin{pmatrix}\n",
    "  W_{.1} \\\\\n",
    "  \\vdots \\\\\n",
    "  W_{.d} \\\\\n",
    " \\end{pmatrix}\n",
    "\\in \\mathbb{R}^{Cd} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Classifiers (For the Win!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax (or logistic regression) alone are not very powerful. As they only generate linear decision boundaries thus, not very helpful for complex problems.\n",
    "\n",
    "<br>\n",
    "\n",
    "Enter NNs - They are able to learn much more complex functions and non-linear decision boundaries.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/decision_boundaries_2.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification difference with word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly in NLP deep learning:\n",
    "* We learn both $W$ and word vectors $x$\n",
    "* We learn both conventional parameters ($W$) and representations ($x$)\n",
    "* Word vectors re-represent one hot vectors, but adjust them in an intermediate layer vector space thus, for easy classification with a (linear) softmax classifier via layer $x = le$ \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\bigtriangledown_\\theta J(\\theta) = \n",
    "\\begin{pmatrix}\n",
    "  W_{.1} \\\\\n",
    "  \\vdots \\\\\n",
    "  W_{.d} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{aardvark} \\\\\n",
    "  \\vdots \\\\\n",
    "  x_{zebra} \\\\\n",
    " \\end{pmatrix}\n",
    "\\in \\mathbb{R}^{Cd+Vd} $$\n",
    "\n",
    "The result being a very large number of paramaters comparative to conventional deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An artificial neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding softmax models means you can understand the operation of neurons\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/neuron.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A neuron can be a binary logistic regression unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f$ = non-linear activation function (e.g. sigmoid), $w$ = weights, $b$ = bias, $h$ = hidden, $x$ = inputs\n",
    "<br>\n",
    "<br>\n",
    "$$h_{w,b}(x) = f(w^Tx+b)$$\n",
    "<br>\n",
    "<br>\n",
    "Sigmoid:\n",
    "$$f(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/neuron_log_regression.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Q: What is z in this?</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Neural networks = running a bunch of logistic regressions at the same time</strong>\n",
    "<br>If we feed a vector of inputs, through a bunch of logistic regression units, we get a vector of outputs.\n",
    "<br><em>However, we don't have to decide ahead of time what attributes these units are trying to predict</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/log_units.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
