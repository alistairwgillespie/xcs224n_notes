{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XCS224N Natural Language Processing with Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CS224N](http://web.stanford.edu/class/cs224n/) / [XCS224N](http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&courseId=93933715) / [Lecture](https://youtu.be/8rXD5-xhemo) / [Slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "* Understand how deep learning methods can be applied to NLP\n",
    "* Understand the challenges faced in NLP and language in general\n",
    "* How to build NLP systems of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Human language is a very slow medium\n",
    "* We have come up with a way of compression to communicate. We assume others know about nuance and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Meaning and Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: <strong>meaning</strong> (Webster Dictionary)\n",
    "* the idea that is represented by a word, phrase, etc.\n",
    "\n",
    "Common linguistic way of thinking of meaning (<strong>denotational semantics</strong>):\n",
    "\n",
    "<em> signifier (symbol) <==> signified (idea or thing) </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common solution in computers is to use <strong>WordNet</strong> - A thesaurus containing lists of synonym sets and hypernyms (\"is a\" relationships).\n",
    "\n",
    "Problems with resources like WordNet:\n",
    "* Great resource but lacks context and nuance\n",
    "* New words or meanings of words may not be captured\n",
    "* Subjective\n",
    "* Requires human labor to maintain\n",
    "* Cannot compute accurate word similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localist Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing words as discrete symbols or otherwise called <strong>localist representation</strong>.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "motel = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
    "hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
    "```\n",
    "Traits:\n",
    "* Vector dimension = number of words in vocabulary (e.g. 500,000)\n",
    "* Matrix will be sparse\n",
    "\n",
    "Limitations of localist representation:\n",
    "* Example like hotel and motel are orthogonal thus there is no notion of similarity when <em>one hot encoding</em>\n",
    "\n",
    "Possible solutions:\n",
    "* WordNet list of synonyms to get similarity. Most likely will fail badly due to incompleteness and other factors\n",
    "* Instead: learn to <em>encode similarity</em> into the vectors themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Distributional Semantics</strong>: A word's meaning is given by the words that frequently appear close-by.\n",
    "\n",
    "The idea is that when a <strong>word w</strong> appears in a text, its <strong>context</strong> is the set of words that appear nearby (within a fixed-size window).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/context_example.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Word vectors</strong>: We build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n",
    "\n",
    "Sometimes called:\n",
    "* Word vectors\n",
    "* Word embeddings\n",
    "* Word representations\n",
    "* Distributed representations\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/word_vector_example.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Word2vec</strong> (Mikolov et al. 2013) is a framework for learning word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider a large corpus of text\n",
    "2. Every word in a fixed vocabulary is represented by a vector\n",
    "3. Iterate through each position <strong>t in the text</strong> , which has a <strong>center word c</strong> and <strong>context words o </strong>\n",
    "4. Use the similarity of the word vectors for <strong>c</strong> and <strong>o</strong> to calculate the probability of <strong>o</strong> given <strong>c</strong> (or vice versa)\n",
    "5. Keep adjusting the word vectors to maximise the probability\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/word2vec_example.PNG\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/word2vec_example2.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec Derivations of Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each position $t = 1,..,T$ predict context words within a window fixed size $m$, given center word $w_j$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Likelihood = L(\\theta) = \\displaystyle\\prod_{t=1}^T \\displaystyle\\prod_{\\substack{-m\\leq j \\leq m \\\\ j \\neq 0}} P(w_{t+j}| w_t; \\theta)$$\n",
    "\n",
    "Objective function\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{2} log L(\\theta)$ = -\\frac{1}{T} \\displaystyle\\sum_{t=1}^T \\displaystyle\\sum_{\\substack{-m\\leq j \\leq m \\\\ j \\neq 0}}logP(w_{t+j}| w_t; \\theta)$$\n",
    "\n",
    "$\\theta$ - All variables to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
